{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the YOLOv8 model for pose estimation\n",
    "model = YOLO('yolov8s-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the video containig all the movements\n",
    "video_path = \"./movements.webm\"\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "seconds = round(frames/fps)\n",
    "\n",
    "#Number of frames to be read\n",
    "frame_total = 1500\n",
    "i = 0\n",
    "a = 0\n",
    "\n",
    "all_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511 1527\n"
     ]
    }
   ],
   "source": [
    "while (cap.isOpened()):\n",
    "  cap.set(cv2.CAP_PROP_POS_MSEC, (i * ((seconds/frame_total)*1000)))\n",
    "  flag, frame = cap.read()\n",
    "\n",
    "  if flag == False:\n",
    "    break\n",
    "\n",
    "  image_path = f'./img_{i}.jpg'\n",
    "  cv2.imwrite(image_path, frame)\n",
    "\n",
    "  # YOLOv8 Will detect your video frame\n",
    "  results = model(frame, verbose=False)\n",
    "\n",
    "  for r in results:\n",
    "    bound_box = r.boxes.xyxy  # get the bounding box on the frame\n",
    "    conf = r.boxes.conf.tolist() # get the confident it is a human from a frame\n",
    "    keypoints = r.keypoints.xyn.tolist() # get the every human keypoint from a frame\n",
    "\n",
    "    # this code for save every human that detected from 1 image, so if 1 image have 10 people, we will save 10 human picture.\n",
    "\n",
    "    for index, box in enumerate(bound_box):\n",
    "      if conf[index] > 0.75: # we do it for reduce blurry human image.\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        pict = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        output_path = f'./person_{a}.jpg'\n",
    "\n",
    "        # we save the person image file name to csv for labelling the csv file.\n",
    "        data = {'image_name': f'person_{a}.jpg'}\n",
    "\n",
    "        # Initialize the x and y lists for each possible key\n",
    "        for j in range(len(keypoints[index])):\n",
    "            data[f'x{j}'] = keypoints[index][j][0]\n",
    "            data[f'y{j}'] = keypoints[index][j][1]\n",
    "\n",
    "      # we save the human keypoint that detected by yolo model to csv file to train our Machine learning model later.\n",
    "\n",
    "        all_data.append(data)\n",
    "        cv2.imwrite(output_path, pict)\n",
    "        a += 1\n",
    "\n",
    "  i += 1\n",
    "\n",
    "print(i-1, a-1)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Combine all data dictionaries into a single DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = './keypoints.csv'\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
